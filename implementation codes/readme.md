Our implementation codes are written in Python with customized C++ extensions, which make use of the Intel MKL library to calculate the quantum simulation. To run the codes, it is required that Pytorch and Numba are installed in the Python environment, and that the system environment variable ```${MKLROOT}``` is set correctly.
We also assume that the GPU based computation, i.e. CUDA, is available. 

In each of the folders above, the main program is the python script ```main_parallel.py```, which can be run as ```python3.x main_parallel.py```. The arguments and settings are organized in file ```arguments.py``` and can be invoked by commandline arguments.

### System Parameters
The system parameters for the quadratic systems are ```gamma``` and the level of the high energy cutoff ```n_max``` (we keep ```omega``` fixed). For the quartic systems the parameters are ```lambda```, ```mass``` and ```gamma``` and half of the size of the simulation space ```x_max```, and the spatial discretization step ```grid_size```. See ```arguments.py``` for the physical units. When the program detects non-negligible amplitude around the energy cutoff or around the boundary of the simulation space, it aborts the simulation, throws out a warning, and goes for a next trial. The number of warnings decreases as the system becomes stabler when the learning proceeds. 

When one changes a system parameter that increases the simulated maximal energy, the number of time steps ```time_steps``` parameter must be increased accordingly to prevent numerical divergence. Namely, it should be proportionally to ```n_max```, or proportional to the maximum between the square of ```grid_size``` and ```lambda``` times the forth of ```x_max```. Note that specifically for the quartic cartpole problem, the spatial discretization must be dense enough so that the wavefunction can obtain a sufficiently short wavelength as it obtains kinetic energy when going off the potential; otherwise it would not be able to accelerate or move away from the center.

Other settings concerned with the reinforcement learning progress are detailed in the notations in the scripts. Note that the learning rate schedule is not included in the commandline arguments and is directly defined in file ```arguments.py```, and the schedule make effects only when the current learning rate is higher.

### Code Organization
Our implementation basically includes three different submodules. The first is the quantum simulation submodule, which is coded in ```simulation.cpp``` and is compiled for Python by invoking the script ```setupC.py```. The second one is the deep learning submodule, which includes the reinforcement learning algorithm, prioritized sampling and the neural networks, which are coded in ```layers.py``` ```optimizer.py``` and ```RL.py```. The last submodule is the manager of the whole system, which controls the workers that carry out the quantum simulation and controls the reinforcement learning progress, which is written in ```main_parallel.py```. 
The script ```main_parallel.py``` first receives and processes the arguments defined in ```arguments.py``` and checks the C++ quantum simulation module, and then it allocates memory and starts parallelized training/testing. For quartic systems, some global definitions are moved to ```space_def.py```.
